// AIDialogue.jsx (ìˆ˜ì •ë³¸)

import {useState, useRef, useEffect, useCallback} from "react";
import KioskLayout from "../../components/layout/KioskLayout";
import MicIcon from "../../assets/icons/mic.svg?react";
import logo from "../../assets/images/logo.png";

// [ì œê±°] import {api} from "../../utils/api"; // â—€ 'window.electronAPI'ë¡œ í†µì¼í•˜ë¯€ë¡œ ì œê±°

// (PrintIcon ì»´í¬ë„ŒíŠ¸... ìƒëµ)
const PrintIcon = () => (
    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" strokeWidth={1.5} stroke="currentColor"
         className="w-8 h-8">
        <path strokeLinecap="round" strokeLinejoin="round"
              d="M6.72 13.829c-.24.03-.48.062-.72.096m.72-.096a42.415 42.415 0 0110.56 0m-10.56 0L6.34 18m10.94-4.171c.24.03-.48.062-.72.096m-.72-.096L17.66 18m0 0l.229 2.523a1.125 1.125 0 01-1.12 1.227H7.231c-.662 0-1.18-.568-1.12-1.227L6.34 18m11.318 0h1.091A2.25 2.25 0 0021 15.75V9.456c0-1.081-.768-2.015-1.837-2.175a48.055 48.055 0 00-1.913-.247M6.34 18H5.25A2.25 2.25 0 013 15.75V9.456c0-1.081.768-2.015 1.837-2.175a48.041 48.041 0 011.913-.247m10.5 0a48.536 48.536 0 00-10.5 0m10.5 0V3.375c0-.621-.504-1.125-1.125-1.125h-8.25c-.621 0-1.125.504-1.125 1.125v3.659M18 10.5h.008v.008H18V10.5zm-3 0h.008v.008H15V10.5z"/>
    </svg>
);

const INTERIM_MESSAGE_ID = "interim-message-id";

export default function AIDialogue({
                                       banner,
                                       setContrastLevel,
                                       zoomLevel,
                                       setZoomLevel,
                                       voiceSettings,
                                       setVoiceSettings,
                                       isSpeaking,
                                       setIsSpeaking
                                   }) {

    const [lang, setLang] = useState(() => {
        const savedLang = localStorage.getItem("app_lang");
        return savedLang || "ko";
    });

    const [messages, setMessages] = useState(() => {
        const initialGreeting = lang === 'ko'
            ? "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            : "Hello! How can I help you?";
        return [{id: crypto.randomUUID(), role: "assistant", content: initialGreeting}];
    });

    const [liveSubtitle, setLiveSubtitle] = useState("ì•ˆë…•í•˜ì„¸ìš”! í•˜ë‚˜ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ëª…ì†Œë‚˜ ì—¬í–‰ ì •ë³´ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.");
    const [isLoading, setIsLoading] = useState(false);
    const [isListening, setIsListening] = useState(false);

    useEffect(() => {
        const handler = () => setLang(localStorage.getItem("app_lang") || "ko");
        window.addEventListener("languagechange", handler);
        return () => window.removeEventListener("languagechange", handler);
    }, []);

    // --- Refs ---
    const chatEndRef = useRef(null);
    const audioContextRef = useRef(null);
    const mediaStreamRef = useRef(null);
    const workletNodeRef = useRef(null);
    const hasSpokenRef = useRef(false);
    const micClickLockRef = useRef(false);

    // ìƒíƒœ ë™ê¸°í™”ë¥¼ ìœ„í•œ Refs
    const isListeningRef = useRef(isListening);
    const isLoadingRef = useRef(isLoading);
    const isSpeakingRef = useRef(isSpeaking);
    const sttResultLockRef = useRef(false);

    // TTS í ê´€ë ¨ Refs
    const ttsQueueRef = useRef([]);
    const isTtsPlayingRef = useRef(false);
    const ttsBufferRef = useRef(""); // (ì˜ì–´ ë²„í¼ë¡œë„ ì‚¬ìš©ë¨)
    const hasPlaybackStartedRef = useRef(false);
    const isAiStreamingRef = useRef(false);

    // [ì œê±°] const sttCooldownRef = useRef(false); // â—€ ì œê±°ë¨

    // --- ìƒíƒœ Ref ë™ê¸°í™” ---
    useEffect(() => {
        isListeningRef.current = isListening;
    }, [isListening]);
    useEffect(() => {
        isLoadingRef.current = isLoading;
    }, [isLoading]);
    useEffect(() => {
        isSpeakingRef.current = isSpeaking;
    }, [isSpeaking]);

    // --- ë…¹ìŒ ì¤‘ì§€ ë¡œì§ ---
    const stopRecording = useCallback(() => {
        if (!mediaStreamRef.current && !audioContextRef.current) {
            return;
        }
        console.log("stopRecording: Stopping audio streams and context...");
        if (workletNodeRef.current) {
            workletNodeRef.current.port.onmessage = null;
            try {
                workletNodeRef.current.disconnect();
            } catch (e) {
                console.warn("Error disconnecting workletNode:", e);
            }
            workletNodeRef.current = null;
        }
        if (audioContextRef.current?.state !== 'closed') {
            try {
                audioContextRef.current?.close();
            } catch (e) {
                console.warn("Error closing AudioContext:", e);
            }
            audioContextRef.current = null;
        }
        if (mediaStreamRef.current) {
            mediaStreamRef.current.getTracks().forEach(track => track.stop());
            mediaStreamRef.current = null;
        }
        console.log(">>> Sending stopSpeechStream to main.js (Physical stop)");
        window.electronAPI.stopSpeechStream();
    }, []);

    // --- TTS í ì¬ìƒ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    const speakNextInQueue = useCallback(() => {
        if (isTtsPlayingRef.current || ttsQueueRef.current.length === 0) {
            return;
        }
        if (!lang.startsWith('ko') && !lang.startsWith('en')) {
            console.log(`TTS Skipped: Unsupported language (${lang}). Clearing queue.`);
            ttsQueueRef.current = [];
            isTtsPlayingRef.current = false;
            if (setIsSpeaking) setIsSpeaking(false);
            isSpeakingRef.current = false;
            hasPlaybackStartedRef.current = false;
            return;
        }
        const textToPlay = ttsQueueRef.current.shift();
        if (!textToPlay || !textToPlay.trim()) {
            speakNextInQueue();
            return;
        }
        isTtsPlayingRef.current = true;
        if (setIsSpeaking) setIsSpeaking(true);
        isSpeakingRef.current = true;
        let targetLang = lang.startsWith('ko') ? 'ko' : 'en';
        const commandObject = {text: textToPlay};
        console.log(`Streaming TTS: Sending to pipe (Lang: ${targetLang}):`, commandObject);
        window.electronAPI.sendTtsCommand(targetLang, commandObject);
    }, [setIsSpeaking, lang]);

    // --- TTS í ì¶”ê°€ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    const addTextToTtsQueue = useCallback((text, forcePlay = false) => {
        if (!text || !text.trim()) return;
        ttsBufferRef.current += text;
        const terminators = /[.!?ã€‚ï¼ï¼Ÿ\n]/;
        let sentenceEndIndex = ttsBufferRef.current.search(terminators);
        while (sentenceEndIndex !== -1) {
            const sentence = ttsBufferRef.current.substring(0, sentenceEndIndex + 1).trim();
            ttsBufferRef.current = ttsBufferRef.current.substring(sentenceEndIndex + 1);
            if (sentence) {
                ttsQueueRef.current.push(sentence);
                console.log("TTS Queue: Added sentence:", sentence);
            }
            sentenceEndIndex = ttsBufferRef.current.search(terminators);
        }
        if (isTtsPlayingRef.current) return;
        const isKo = lang.startsWith('ko');
        const minSentencesToStart = isKo ? 1 : 5;
        if (forcePlay && ttsQueueRef.current.length > 0) {
            hasPlaybackStartedRef.current = true;
            speakNextInQueue();
        } else if (hasPlaybackStartedRef.current && ttsQueueRef.current.length > 0) {
            speakNextInQueue();
        } else if (!hasPlaybackStartedRef.current && ttsQueueRef.current.length >= minSentencesToStart) {
            console.log(`TTS Start Condition MET. (Lang: ${isKo ? 'ko' : 'other'}, Queue: ${ttsQueueRef.current.length})`);
            hasPlaybackStartedRef.current = true;
            speakNextInQueue();
        }
    }, [speakNextInQueue, lang]); // â—€ lang ì˜ì¡´ì„± ì¶”ê°€

    // --- TTS ë²„í¼ í”ŒëŸ¬ì‹œ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    const flushTtsBuffer = useCallback(() => {
        const leftoverText = ttsBufferRef.current.trim();
        if (leftoverText) {
            ttsQueueRef.current.push(leftoverText);
        }
        ttsBufferRef.current = "";
        if (!isTtsPlayingRef.current && ttsQueueRef.current.length > 0) {
            hasPlaybackStartedRef.current = true;
            speakNextInQueue();
        }
    }, [speakNextInQueue]);

    // --- TTS ì¤‘ì§€ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    const stopAndClearTtsQueue = useCallback(() => {
        console.log("Streaming TTS: Stopping all playback and clearing queue.");
        ttsQueueRef.current = [];
        ttsBufferRef.current = "";
        isTtsPlayingRef.current = false;
        if (setIsSpeaking) setIsSpeaking(false);
        isSpeakingRef.current = false;
        hasPlaybackStartedRef.current = false;
        isAiStreamingRef.current = false;
        window.electronAPI.sendTtsCommand('ALL', {command: "stop"});
    }, [setIsSpeaking]);

    // --- TTS ì¬ìƒ ì™„ë£Œ ë¦¬ìŠ¤ë„ˆ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    useEffect(() => {
        const removeListener = window.electronAPI.onTtsPlaybackFinished(() => {
            console.log(">>> onTtsPlaybackFinished received.");
            isTtsPlayingRef.current = false;
            if (ttsQueueRef.current.length > 0) {
                speakNextInQueue();
            } else {
                if (!isAiStreamingRef.current) {
                    hasPlaybackStartedRef.current = false;
                    if (setIsSpeaking) setIsSpeaking(false);
                    isSpeakingRef.current = false;
                }
            }
        });
        return () => removeListener();
    }, [speakNextInQueue, setIsSpeaking]);

    // --- ë…¹ìŒ ì‹œì‘ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    const startRecording = useCallback(async () => {
        if (isListeningRef.current) return;
        if (isSpeakingRef.current) return;
        sttResultLockRef.current = false;
        console.log("Attempting to start recording...");
        setIsListening(true);
        isListeningRef.current = true; // â—€ "ì¦‰ì‹œ ë™ê¸°í™”" (1)
        try {
            let selectedDeviceId = null;
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputDevices = devices.filter(device => device.kind === 'audioinput');
                selectedDeviceId = audioInputDevices[0].deviceId;
            } catch (err) {
                throw new Error(`ë§ˆì´í¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: ${err.message}`);
            }
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: selectedDeviceId ? {
                    deviceId: {exact: selectedDeviceId}, sampleRate: 16000
                } : {sampleRate: 16000},
                video: false
            });
            mediaStreamRef.current = stream;
            const context = new AudioContext({sampleRate: 16000});
            audioContextRef.current = context;
            try {
                await context.audioWorklet.addModule('audio-processor.js');
            } catch (e) {
                throw new Error(`audio-processor.js ë¡œë“œ ì‹¤íŒ¨: ${e.message}`);
            }
            const source = context.createMediaStreamSource(stream);
            const workletNode = new AudioWorkletNode(context, 'audio-processor');
            workletNodeRef.current = workletNode;
            workletNode.port.onmessage = (event) => {
                if (!workletNodeRef.current || !isListeningRef.current) return;
                if (event.data instanceof Float32Array) {
                    const float32Array = event.data;
                    const int16Array = new Int16Array(float32Array.length);
                    for (let i = 0; i < float32Array.length; i++) {
                        int16Array[i] = Math.max(-1, Math.min(1, float32Array[i])) * 0x7FFF;
                    }
                    window.electronAPI.sendAudioChunk(int16Array.buffer);
                }
            };
            source.connect(workletNode);
            window.electronAPI.startSpeechStream(lang);
        } catch (err) {
            console.error("Error during startRecording:", err);
            alert(`ë§ˆì´í¬ ì—ëŸ¬: ${err.message}`);
            setIsListening(false);
            isListeningRef.current = false; // â—€ "ì¦‰ì‹œ ë™ê¸°í™”" (2)
        }
    }, [setIsListening, lang]);

    // --- [ìˆ˜ì •] submitMessage (ë¹„(é)-STT, ì¦‰ íƒ€ì´í•‘ ì…ë ¥ìš©) ---
    const submitMessage = useCallback(async (messageText) => {
        if (!messageText || !messageText.trim() || isLoadingRef.current) return;
        console.log(">>> submitMessage (Streaming) called with:", messageText);
        stopAndClearTtsQueue();
        setIsLoading(true);

        const userMessage = {id: crypto.randomUUID(), role: "user", content: messageText};

        const newMessagesForApi = await new Promise(resolve => {
            setMessages(currentMessages => {
                const filteredMessages = currentMessages.filter(m => m.id !== INTERIM_MESSAGE_ID);
                const newMessages = [
                    ...filteredMessages,
                    userMessage,
                    {id: crypto.randomUUID(), role: "assistant", content: ""}
                ];
                // APIì—ëŠ” 'userMessage'ë¥¼ í¬í•¨í•œ íˆìŠ¤í† ë¦¬ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
                resolve([...filteredMessages, userMessage].map(m => ({ role: m.role, content: m.content })));
                return newMessages;
            });
        });

        try {
            isAiStreamingRef.current = true;


            // ğŸ”½ [ìˆ˜ì •] 'api.askAI' -> 'window.electronAPI.askAI'
            window.electronAPI.askAI(newMessagesForApi);
        } catch (error) {
            console.error("AI ìš”ì²­ ì „ì†¡ ì˜¤ë¥˜:", error);
            setMessages(prev => {
                const lastMsg = prev[prev.length - 1];
                if (lastMsg && lastMsg.role === 'assistant') {
                    return [...prev.slice(0, -1), {...lastMsg, content: "AI ìš”ì²­ ì‹¤íŒ¨."}];
                }
                return [...prev, {id: crypto.randomUUID(), role: "assistant", content: "AI ìš”ì²­ ì‹¤íŒ¨."}];
            });
            if (setIsSpeaking) setIsSpeaking(false);
            setIsLoading(false);
            isAiStreamingRef.current = false;
        }
    }, [setIsSpeaking, setIsLoading, setMessages, stopAndClearTtsQueue]);


    // --- [ì‹ ê·œ] submitSttMessage (STT ì…ë ¥ ì „ìš©) ---
    /**
     * STT ìµœì¢… ê²°ê³¼ë¥¼ ë°±ì—”ë“œ(handleUserSttInput)ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
     */
    const submitSttMessage = useCallback(async (sttText) => {
        if (!sttText || !sttText.trim() || isLoadingRef.current) {
            console.warn("submitSttMessage: ì¤‘ë³µ í˜¸ì¶œ ë˜ëŠ” ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ ë¬´ì‹œë¨.");
            return;
        }

        console.log(">>> submitSttMessage (STT Flow) called with:", sttText);

        // 1. ê¸°ì¡´ TTS ì¤‘ì§€, ë¡œë”© ìƒíƒœ ì‹œì‘
        stopAndClearTtsQueue();
        setIsLoading(true);

        const userMessage = {id: crypto.randomUUID(), role: "user", content: sttText};

        // 2. 'ì´ì „' ëŒ€í™” ë‚´ì—­ (API ì „ì†¡ìš©)
        // setMessages ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìµœì‹  ìƒíƒœ(prev)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
        // 'ì´ì „' ëŒ€í™” ë‚´ì—­ì„ ê°€ì ¸ì˜¤ê³ , 'ì´í›„' UI ìƒíƒœë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        const conversationHistory = await new Promise(resolve => {
            setMessages(prev => {
                const filteredMessages = prev.filter(m => m.id !== INTERIM_MESSAGE_ID);

                // APIë¡œ ë³´ë‚¼ 'ì´ì „' ëŒ€í™” ë‚´ì—­ (role, contentë§Œ)
                const historyForApi = filteredMessages
                    .filter(m => m.role === 'user' || m.role === 'assistant')
                    .map(({role, content}) => ({role, content}));

                resolve(historyForApi); // Promiseì— 'ì´ì „' ë‚´ì—­ ë°˜í™˜

                // UIì— í‘œì‹œí•  'ìƒˆ' ìƒíƒœ (ìœ ì € ë©”ì‹œì§€ + AI ì…¸)
                return [
                    ...filteredMessages,
                    userMessage,
                    {id: crypto.randomUUID(), role: "assistant", content: ""}
                ];
            });
        });

        // 3. ìƒˆ IPC ì±„ë„ë¡œ ì „ì†¡
        try {
            isAiStreamingRef.current = true;

            console.log(`[STT Submit] Sending to main: (Text: "${sttText}", History Length: ${conversationHistory.length})`);

            // ğŸ”½ [ì‹ ê·œ] ìƒˆë¡œìš´ 'submitSttForAI' API í˜¸ì¶œ
            // ë°±ì—”ë“œê°€ STT êµì •, AI í˜¸ì¶œ, ìŠ¤íŠ¸ë¦¬ë°ì„ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            window.electronAPI.submitSttForAI(sttText, conversationHistory,lang);

        } catch (error) {
            console.error("STT ìš”ì²­ ì „ì†¡ ì˜¤ë¥˜:", error);
            setMessages(prev => {
                const lastMsg = prev[prev.length - 1];
                if (lastMsg && lastMsg.role === 'assistant') {
                    return [...prev.slice(0, -1), {...lastMsg, content: "STT ìš”ì²­ ì‹¤íŒ¨."}];
                }
                return [...prev, {id: crypto.randomUUID(), role: "assistant", content: "STT ìš”ì²­ ì‹¤íŒ¨."}];
            });
            if (setIsSpeaking) setIsSpeaking(false);
            setIsLoading(false);
            isAiStreamingRef.current = false;
        }

    }, [setIsSpeaking, setIsLoading, setMessages, stopAndClearTtsQueue]);
    // --- [ì‹ ê·œ] ë ---


    // --- í•¨ìˆ˜ Ref í™” (STT ë¦¬ìŠ¤ë„ˆì—ì„œ ìµœì‹  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•¨) ---
    const addTextToTtsQueueRef = useRef(addTextToTtsQueue);
    const flushTtsBufferRef = useRef(flushTtsBuffer);
    const setIsLoadingRef = useRef(setIsLoading);
    const setMessagesRef = useRef(setMessages);
    const submitMessageRef = useRef(submitMessage);
    const submitSttMessageRef = useRef(submitSttMessage); // â—€ [ì‹ ê·œ] STT ì „ìš© submit Ref
    const setIsListeningRef = useRef(setIsListening);
    const stopRecordingRef = useRef(stopRecording);
    const stopAndClearTtsQueueRef = useRef(stopAndClearTtsQueue); // â—€ [ì‹ ê·œ] STT ê²°ê³¼ ì²˜ë¦¬ì‹œ TTS ì¤‘ì§€ë¥¼ ìœ„í•´ ì¶”ê°€

    // --- í•¨ìˆ˜ Ref ìµœì‹ í™” ---
    useEffect(() => {
        addTextToTtsQueueRef.current = addTextToTtsQueue;
    }, [addTextToTtsQueue]);
    useEffect(() => {
        flushTtsBufferRef.current = flushTtsBuffer;
    }, [flushTtsBuffer]);
    useEffect(() => {
        setIsLoadingRef.current = setIsLoading;
    }, [setIsLoading]);
    useEffect(() => {
        setMessagesRef.current = setMessages;
    }, [setMessages]);
    useEffect(() => {
        submitMessageRef.current = submitMessage;
    }, [submitMessage]);
    useEffect(() => { // â—€ [ì‹ ê·œ]
        submitSttMessageRef.current = submitSttMessage;
    }, [submitSttMessage]);
    useEffect(() => {
        setIsListeningRef.current = setIsListening;
    }, [setIsListening]);
    useEffect(() => {
        stopRecordingRef.current = stopRecording;
    }, [stopRecording]);
    useEffect(() => { // â—€ [ì‹ ê·œ]
        stopAndClearTtsQueueRef.current = stopAndClearTtsQueue;
    }, [stopAndClearTtsQueue]);


    // ì´ˆê¸° ë©”ì‹œì§€ ì¬ìƒ (ê¸°ì¡´ê³¼ ë™ì¼)
    useEffect(() => {
        if (typeof setIsSpeaking !== "function") return;
        if (hasSpokenRef.current) return;
        const initialMessage = messages[0]?.content;
        if (!initialMessage) return;
        const timerId = setTimeout(() => {
            hasSpokenRef.current = true;
            addTextToTtsQueueRef.current(initialMessage, true);
        }, 50);
        return () => clearTimeout(timerId);
    }, [messages, setIsSpeaking]);

    // í™”ë©´ ì´íƒˆ ì‹œ ì¤‘ì§€ (ê¸°ì¡´ê³¼ ë™ì¼)
    useEffect(() => {
        return () => {
            console.log("í™”ë©´ ì´íƒˆ: TTS ë° ë…¹ìŒ ì¤‘ì§€");
            stopAndClearTtsQueue();
            stopRecording();
        };
    }, [stopRecording, stopAndClearTtsQueue]);


    // --- ğŸ”½ [ìˆ˜ì •] STT ê²°ê³¼/ì—ëŸ¬ ì²˜ë¦¬ ---
    useEffect(() => {
        // ì¤‘ê°„ ê²°ê³¼ ë¦¬ìŠ¤ë„ˆ (ê¸°ì¡´ê³¼ ë™ì¼)
        const removeInterimListener = window.electronAPI.onSpeechInterimResult((transcript) => {
            if (!isListeningRef.current || isSpeakingRef.current) {
                return;
            }
            setMessagesRef.current(prev => {
                const lastMsg = prev[prev.length - 1];
                if (lastMsg && lastMsg.id === INTERIM_MESSAGE_ID) {
                    return [...prev.slice(0, -1), {id: INTERIM_MESSAGE_ID, role: "interim", content: transcript}];
                } else {
                    return [...prev, {id: INTERIM_MESSAGE_ID, role: "interim", content: transcript}];
                }
            });
        });

        // [ìˆ˜ì •] ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤ë„ˆ
        const removeResultListener = window.electronAPI.onSpeechResult(async (transcript) => {
            console.log(">>> onSpeechResult (Final) received:", transcript);

            // ğŸ”½ "ì ê¸ˆ" (ê¸°ì¡´ê³¼ ë™ì¼)
            if (!isListeningRef.current || isSpeakingRef.current) {
                console.warn("onSpeechResult: Ignoring stale transcript (Guard ACTIVE: isListening=false or AI isSpeaking).");
                return;
            }

            if (!transcript || !transcript.trim()) {
                console.warn("onSpeechResult: Empty transcript ignored.");
                // [ìˆ˜ì •] ë¹ˆ ê²°ê³¼ ìˆ˜ì‹  ì‹œ interim ì œê±° ë° ë…¹ìŒ ì¤‘ì§€
                setMessagesRef.current(prev => prev.filter(m => m.id !== INTERIM_MESSAGE_ID));
                setIsListeningRef.current(false);
                isListeningRef.current = false; // "ì¦‰ì‹œ ë™ê¸°í™”"
                stopRecordingRef.current();
                return;
            }

            if (sttResultLockRef.current) {
                console.warn("onSpeechResult: STT result is already being processed. Ignoring duplicate.");
                return;
            }

            // [ìˆ˜ì •] UIì—ì„œ 'interim' ë©”ì‹œì§€ ì¦‰ì‹œ ì œê±°
            setMessagesRef.current(prev => prev.filter(m => m.id !== INTERIM_MESSAGE_ID));

            try {
                sttResultLockRef.current = true;
                console.log("onSpeechResult: Acquired STT lock, processing result.");

                // [ìˆ˜ì •] ë…¹ìŒ ì¤‘ì§€ ë° ìƒíƒœ ë™ê¸°í™”
                setIsListeningRef.current(false);
                isListeningRef.current = false; // "ì¦‰ì‹œ ë™ê¸°í™”" (3)
                stopRecordingRef.current();

                // ğŸ”½ [ìˆ˜ì •] ìƒˆë¡œìš´ STT ì „ìš© submit í•¨ìˆ˜ í˜¸ì¶œ
                // ì´ í•¨ìˆ˜ê°€ ë¡œë”© ì„¤ì •, UI ì—…ë°ì´íŠ¸, API í˜¸ì¶œì„ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                submitSttMessageRef.current(transcript);

                // ğŸ”½ [ì œê±°] ê¸°ì¡´ ë¡œì§ (ìˆ˜ë™ êµì •, ìˆ˜ë™ submit)
                // const correctedText = await window.electronAPI.correctSTT(transcript);
                // submitMessageRef.current(correctedText);

            } catch (error) {
                // [ìˆ˜ì •] submitSttMessageRef.current(transcript) í˜¸ì¶œì—ì„œ ì—ëŸ¬ê°€ ë‚  ê²½ìš°
                // (ì´ë¡ ìƒ 'submitSttMessage' ë‚´ë¶€ì˜ try/catchê°€ ì²˜ë¦¬í•´ì•¼ í•¨)
                // ë§Œì•½ì˜ ì‚¬íƒœë¥¼ ëŒ€ë¹„í•´ 'submitMessage' (ê¸°ì¡´ ë¡œì§)ë¡œ í´ë°±í•©ë‹ˆë‹¤.
                console.error("onSpeechResult: Fallback error:", error);
                submitMessageRef.current(transcript); // í´ë°± (ê¸°ì¡´ 'openai:ask' ì‚¬ìš©)

            } finally {
                // [ìˆ˜ì •] ë½ í•´ì œ
                // submitSttMessageê°€ API í˜¸ì¶œì„ 'ì „ì†¡'ë§Œ í•˜ë¯€ë¡œ ë½ì„ ì¦‰ì‹œ í•´ì œí•©ë‹ˆë‹¤.
                // ë¡œë”© ìƒíƒœ(isLoading)ëŠ” onAIStreamEndì—ì„œ í•´ì œë©ë‹ˆë‹¤.
                sttResultLockRef.current = false;
            }
        });

        // ì—ëŸ¬ ë¦¬ìŠ¤ë„ˆ (ê¸°ì¡´ê³¼ ë™ì¼)
        const removeErrorListener = window.electronAPI.onSpeechError((error) => {
            console.error(">>> onSpeechError received:", error);
            if (!isListeningRef.current || isSpeakingRef.current) {
                console.warn("onSpeechError: Ignoring stale error (Guard ACTIVE: isListening=false or AI isSpeaking).");
                return;
            }
            setMessagesRef.current(prev => prev.filter(m => m.id !== INTERIM_MESSAGE_ID));
            sttResultLockRef.current = false;
            setIsListeningRef.current(false);
            isListeningRef.current = false; // "ì¦‰ì‹œ ë™ê¸°í™”" (4)
            stopRecordingRef.current();
        });

        return () => {
            removeInterimListener();
            removeResultListener();
            removeErrorListener();
        };
    }, []); // â—€ ì˜ì¡´ì„± ë°°ì—´ [] ìœ ì§€ (ëª¨ë“  í•¨ìˆ˜ëŠ” Refë¥¼ í†µí•´ í˜¸ì¶œë¨)
    // --- [ìˆ˜ì •] STT ê²°ê³¼ ì²˜ë¦¬ ë ---


    // ìŠ¤í¬ë¡¤ (ê¸°ì¡´ê³¼ ë™ì¼)
    useEffect(() => {
        chatEndRef.current?.scrollIntoView({behavior: "smooth"});
    }, [messages]);


    // --- ğŸ”½ [ìˆ˜ì •] AI ìŠ¤íŠ¸ë¦¬ë° ë¦¬ìŠ¤ë„ˆ ('api.' -> 'window.electronAPI.') ---
    useEffect(() => {
        const handleAIChunk = (chunk) => {
            if (!chunk) return;
            if (lang.startsWith('en')) {
                ttsBufferRef.current += chunk;
                return;
            }
            isAiStreamingRef.current = true;
            setMessagesRef.current(prev => {
                const lastAsstMsgIndex = prev.findLastIndex(m => m.role === 'assistant');
                if (lastAsstMsgIndex !== -1) {
                    const newMessages = [...prev];
                    const newContent = (newMessages[lastAsstMsgIndex].content || "") + chunk;
                    newMessages[lastAsstMsgIndex] = {...prev[lastAsstMsgIndex], content: newContent};
                    return newMessages;
                }
                return prev;
            });
            setTimeout(() => {
                addTextToTtsQueueRef.current(chunk, false);
            }, 0);
        };

        const handleAIStreamEnd = () => {
            console.log("Streaming: END signal received.");
            if (lang.startsWith('en')) {
                const fullText = ttsBufferRef.current;
                ttsBufferRef.current = "";
                if (fullText.trim()) {
                    setMessagesRef.current(prev => {
                        const lastAsstMsgIndex = prev.findLastIndex(m => m.role === 'assistant');
                        if (lastAsstMsgIndex !== -1) {
                            const newMessages = [...prev];
                            newMessages[lastAsstMsgIndex] = {...prev[lastAsstMsgIndex], content: fullText};
                            return newMessages;
                        }
                        return prev;
                    });
                    addTextToTtsQueueRef.current(fullText, true);
                }
            } else {
                flushTtsBufferRef.current();
            }
            setIsLoadingRef.current(false);
            isAiStreamingRef.current = false;

            if (ttsQueueRef.current.length === 0 && !isTtsPlayingRef.current) {
                hasPlaybackStartedRef.current = false;
                if (setIsSpeaking) setIsSpeaking(false);
                isSpeakingRef.current = false;
            }
        };

        const handleAIError = (errorMsg) => {
            console.error("Streaming: ERROR received:", errorMsg);
            setIsLoadingRef.current(false);
            ttsBufferRef.current = "";
            setMessagesRef.current(prev => {
                const lastMsg = prev[prev.length - 1];
                if (lastMsg && lastMsg.role === 'assistant' && lastMsg.content === "") {
                    return [...prev.slice(0, -1), {...lastMsg, id: crypto.randomUUID(), content: errorMsg}];
                }
                return [...prev, {id: crypto.randomUUID(), role: "assistant", content: errorMsg}];
            });
            isAiStreamingRef.current = false;
            hasPlaybackStartedRef.current = false;
        };

        // ğŸ”½ [ìˆ˜ì •] 'api.' -> 'window.electronAPI.'
        const removeChunkListener = window.electronAPI.onAIChunk(handleAIChunk);
        const removeEndListener = window.electronAPI.onAIStreamEnd(handleAIStreamEnd);
        const removeErrorListener = window.electronAPI.onAIError(handleAIError);

        return () => {
            removeChunkListener();
            removeEndListener();
            removeErrorListener();
        };
    }, [setIsSpeaking, lang]); // â—€ lang ì˜ì¡´ì„± ìœ ì§€
    // --- [ìˆ˜ì •] AI ìŠ¤íŠ¸ë¦¬ë° ë¦¬ìŠ¤ë„ˆ ë ---


    // PTT ë¡œì§ (ë§ˆì´í¬ í´ë¦­, ê¸°ì¡´ê³¼ ë™ì¼)
    const handleMicClick = useCallback(() => {
        if (micClickLockRef.current) return;
        micClickLockRef.current = true;
        setTimeout(() => {
            micClickLockRef.current = false;
        }, 300);

        const currentIsSpeaking = isSpeakingRef.current;
        const currentIsListening = isListeningRef.current;

        if (currentIsListening) {
            console.log("Action: (PTT) Stopping recording.");
            sttResultLockRef.current = false;
            setIsListening(false);
            isListeningRef.current = false; // â—€ "ì¦‰ì‹œ ë™ê¸°í™”" (5)
            stopRecording();
            setMessages(prev => prev.filter(m => m.id !== INTERIM_MESSAGE_ID));
        } else if (currentIsSpeaking) {
            console.log("Action: (PTT) Interrupting TTS and starting recording.");
            stopAndClearTtsQueue();
            startRecording();
        } else {
            console.log("Action: (PTT) Starting recording.");
            stopAndClearTtsQueue();
            startRecording();
        }
    }, [startRecording, stopRecording, setIsListening, stopAndClearTtsQueue, setMessages]);


    // --- ğŸ”½ [ìˆ˜ì •] í”„ë¦°íŠ¸ í•¸ë“¤ëŸ¬ ('api.' -> 'window.electronAPI.') ---
    const handlePrintSingleMessage = async (contentToPrint) => {
        try {
            // ğŸ”½ [ìˆ˜ì •] 'api.' -> 'window.electronAPI.'
            const result = await window.electronAPI.print(contentToPrint);
            if (!result.success) alert(`ì¸ì‡„ ì‹¤íŒ¨: ${result.error}`);
        } catch (error) {
            alert("ì¸ì‡„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
        }
    };


    // JSX (ê¸°ì¡´ê³¼ ë™ì¼)
    return (
        <KioskLayout
            logo={logo} banner={banner} setContrastLevel={setContrastLevel}
            zoomLevel={zoomLevel} setZoomLevel={setZoomLevel}
            voiceSettings={voiceSettings} setVoiceSettings={setVoiceSettings}
            showSubtitle={true}
            subtitle={liveSubtitle}
            setLiveSubtitle={setLiveSubtitle}
        >
            <div className={`w-full max-w-[900px] h-[1300px] relative rounded-xl overflow-hidden`}>
                <div className="h-full overflow-y-auto p-10 space-y-6 pt-40">

                    {messages.map((msg, idx) => (
                        <div key={msg.id} className={`flex items-end gap-4 ${
                            msg.role === 'user' || msg.role === 'interim' ? 'justify-end' : 'justify-start'
                        }`}>
                            {msg.role === 'assistant' && idx > 0 && (
                                <button
                                    onClick={() => handlePrintSingleMessage(msg.content)}
                                    className="p-3 mb-2 rounded-full text-gray-500 hover:bg-gray-200 active:bg-gray-300 focus:outline-none focus:ring-4 focus:ring-blue-500"
                                    title="ì´ ë‹µë³€ ì¸ì‡„í•˜ê¸°"
                                >
                                    <PrintIcon/>
                                </button>
                            )}
                            <div
                                className={`relative p-8 rounded-2xl max-w-[75%] text-[2rem] font-medium leading-relaxed ${
                                    msg.role === "user" ? "bg-blue-500 text-white bubble-user" :
                                        msg.role === "interim" ? "bg-blue-300 text-black bubble-user animate-pulse" :
                                            "bg-gray-200 text-black bubble-ai whitespace-pre-wrap"
                                }`}
                            >
                                {msg.content}
                                {msg.role === "user" ? (
                                    <div
                                        className="bubble-tail-user absolute -right-2 bottom-4 w-0 h-0 border-l-[16px] border-l-blue-500 border-t-[12px] border-t-transparent border-b-[12px] border-b-transparent"></div>
                                ) : msg.role === "interim" ? (
                                    <div
                                        className="bubble-tail-user absolute -right-2 bottom-4 w-0 h-0 border-l-[16px] border-l-blue-300 border-t-[12px] border-t-transparent border-b-[12px] border-b-transparent"></div>
                                ) : (
                                    <div
                                        className="bubble-tail-ai absolute -left-2 bottom-4 w-0 h-0 border-r-[16px] border-r-gray-200 border-t-[12px] border-t-transparent border-b-[12px] border-b-transparent"></div>
                                )}
                            </div>
                        </div>
                    ))}
                    <div ref={chatEndRef}/>
                </div>

                {isSpeaking && !isListening && (
                    <div
                        className="absolute bottom-60 left-1/2 -translate-x-1/2 bg-black bg-opacity-70 text-white px-6 py-3 rounded-full">
                        <p className="text-2xl animate-pulse">ë‹µë³€ì¤‘...</p>
                    </div>
                )}
                {isLoading && (
                    <div
                        className="absolute bottom-60 left-1/2 -translate-x-1/2 bg-yellow-600 bg-opacity-80 text-white px-6 py-3 rounded-full">

                    </div>
                )}

                <button
                    onClick={handleMicClick}
                    disabled={isLoading}
                    className={`absolute bottom-16 left-1/2 -translate-x-1/2 w-40 h-40 rounded-full text-white flex items-center justify-center shadow-lg hover:opacity-90 active:opacity-80 z-10 transition-colors focus:outline-none focus:ring-4 focus:ring-blue-500 ${
                        isListening ? 'bg-red-600 hover:bg-red-700' : 'bg-blue-600 hover:bg-blue-700'
                    } ${isLoading ? 'opacity-50 cursor-not-allowed' : ''}`}
                >
                    <MicIcon className="w-20 h-20"/>
                </button>
            </div>
        </KioskLayout>
    );
}